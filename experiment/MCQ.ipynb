{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2 + 3 is the sum of the numbers 2 and 3. The result of this simple arithmetic operation is 5. In mathematical terms, you can write this as an equation: 2 + 3 = 5. This equation means that when you add the numbers 2 and 3 together, the sum is 5.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5 + 7 = 12\n",
      "\n",
      "Explanation:\n",
      "To find the sum of two numbers, you add the digits in each column, starting from the rightmost column. In this case, we have:\n",
      "\n",
      "```\n",
      " 5\n",
      "+ 7\n",
      "---------\n",
      "12\n",
      "```\n",
      "\n",
      "Therefore, the value of 5 + 7 is 12.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"inp\"],\n",
    "    template=\"What is value of 5{inp}7?\"\n",
    ")\n",
    "\n",
    "# Initialize the HuggingFaceEndpoint model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    temperature=0.7  # Explicit temperature setting\n",
    ")\n",
    "\n",
    "# Create the LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Execute the chain with dynamic input\n",
    "result = chain.run({\"inp\": \"+\"})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parthiv\\AppData\\Local\\Temp\\ipykernel_2512\\466414767.py:33: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=False)\n",
      "C:\\Users\\Parthiv\\AppData\\Local\\Temp\\ipykernel_2512\\466414767.py:68: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = quiz_chain.run({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"1\": {\"mcq\": \"What is the scientific study of?\", \"options\": {\"a\": \"Physics\", \"b\": \"Chemistry\", \"c\": \"Biology\", \"d\": \"Geology\"}, \"correct\": \"c\"},\n",
      "\"2\": {\"mcq\": \"What is the major theme of Biology?\", \"options\": {\"a\": \"Energy processing\", \"b\": \"Cellular structure\", \"c\": \"Evolution\", \"d\": \"Population dynamics\"}, \"correct\": \"c\"},\n",
      "\"3\": {\"mcq\": \"Which of the following organisms is not eukaryotic?\", \"options\": {\"a\": \"Plant\", \"b\": \"Fungi\", \"c\": \"Archaea\", \"d\": \"Animal\"}, \"correct\": \"c\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Define the template for the MCQ generation prompt\n",
    "TEMPLATE = \"\"\"\n",
    "Text: {text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be conforming to the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \\\n",
    "Ensure to make {number} MCQs.\n",
    "here is my response structure, i want response to be in json with given structure.\n",
    "Strictly don't give any other text other than given below response_json format in response\n",
    "{response_json}\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")\n",
    "\n",
    "# Initialize the HuggingFaceEndpoint model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    temperature=0.7  # Explicit temperature setting\n",
    ")\n",
    "\n",
    "# Create the LLM chain\n",
    "quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=False)\n",
    "\n",
    "# Read the text file containing the input text\n",
    "file_path = r\"D:\\AI\\MCQ_generator\\data.txt\"  # Adjust the path to your file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    TEXT = file.read()\n",
    "\n",
    "# Define other parameters for the quiz\n",
    "NUMBER = 3  # Number of MCQs to generate\n",
    "SUBJECT = \"biology\"  # Subject for the quiz\n",
    "TONE = \"simple\"  # Tone of the quiz\n",
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the quiz generation chain\n",
    "response = quiz_chain.run({\n",
    "    \"text\": TEXT,\n",
    "    \"number\": NUMBER,\n",
    "    \"subject\": SUBJECT,\n",
    "    \"tone\": TONE,\n",
    "    \"response_json\": json.dumps(RESPONSE_JSON)  # Format the response JSON as a string\n",
    "})\n",
    "\n",
    "print(response)\n",
    "\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
